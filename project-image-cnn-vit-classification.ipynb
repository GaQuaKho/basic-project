{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nfrom tensorflow.keras.utils import to_categorical\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.datasets import cifar10\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-06T14:44:54.688382Z","iopub.execute_input":"2023-04-06T14:44:54.689925Z","iopub.status.idle":"2023-04-06T14:44:54.696759Z","shell.execute_reply.started":"2023-04-06T14:44:54.689872Z","shell.execute_reply":"2023-04-06T14:44:54.695523Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"(X_train, y_train), (X_test, y_test) = cifar10.load_data()","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:44:57.569200Z","iopub.execute_input":"2023-04-06T14:44:57.569596Z","iopub.status.idle":"2023-04-06T14:45:06.749339Z","shell.execute_reply.started":"2023-04-06T14:44:57.569562Z","shell.execute_reply":"2023-04-06T14:45:06.748279Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 6s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = torch.reshape(torch.from_numpy(X_train), (X_train.shape[0],3,32,32))\nlabel = torch.flatten(torch.from_numpy(y_train))\nlabel = to_categorical(label)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:45:10.602131Z","iopub.execute_input":"2023-04-06T14:45:10.602609Z","iopub.status.idle":"2023-04-06T14:45:11.046185Z","shell.execute_reply.started":"2023-04-06T14:45:10.602562Z","shell.execute_reply":"2023-04-06T14:45:11.045047Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(dataset, label, test_size=0.2)\n\ny_train = torch.from_numpy(y_train)\ny_test = torch.from_numpy(y_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:45:15.080649Z","iopub.execute_input":"2023-04-06T14:45:15.081238Z","iopub.status.idle":"2023-04-06T14:45:15.723616Z","shell.execute_reply.started":"2023-04-06T14:45:15.081199Z","shell.execute_reply":"2023-04-06T14:45:15.722570Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:49:18.625281Z","iopub.execute_input":"2023-04-06T14:49:18.625979Z","iopub.status.idle":"2023-04-06T14:49:18.631439Z","shell.execute_reply.started":"2023-04-06T14:49:18.625940Z","shell.execute_reply":"2023-04-06T14:49:18.630362Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"torch.Size([40000, 3, 32, 32])\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = TensorDataset(x_train, y_train)\ntrain_iter = DataLoader(train_data, batch_size = 512, shuffle=True)\n\ntest_data = TensorDataset(x_test, y_test)\ntest_iter = DataLoader(test_data, batch_size = 512, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T15:32:58.476147Z","iopub.execute_input":"2023-04-06T15:32:58.476546Z","iopub.status.idle":"2023-04-06T15:32:58.483136Z","shell.execute_reply.started":"2023-04-06T15:32:58.476510Z","shell.execute_reply":"2023-04-06T15:32:58.481807Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-04-06T14:45:20.010144Z","iopub.execute_input":"2023-04-06T14:45:20.013240Z","iopub.status.idle":"2023-04-06T14:45:20.081707Z","shell.execute_reply.started":"2023-04-06T14:45:20.013191Z","shell.execute_reply":"2023-04-06T14:45:20.080481Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class ClassificationImage(nn.Module):\n    def __init__(self):\n        super(ClassificationImage,self).__init__()\n        self.conv_2d_1 = nn.Conv2d(3,8,3,padding=1)\n        self.max_1 = nn.MaxPool2d(2, stride=2)\n        self.conv_2d_2 = nn.Conv2d(8,16,3,padding=1)\n        \n        self.flatten = nn.Flatten()\n        self.linear_1 = nn.Linear(1024,32)  \n        self.batch_norm_1 = nn.BatchNorm1d(32)\n\n        self.linear_2 = nn.Linear(32,16)        \n        self.batch_norm_2 = nn.BatchNorm1d(16)\n        self.linear_3 = nn.Linear(16,10)        \n        self.batch_norm_3 = nn.BatchNorm1d(10)\n\n\n        self.softmax = nn.Softmax(dim=1)\n    def forward(self,image):\n\n        h = self.conv_2d_1(image)\n        h = F.leaky_relu(h,0.01)\n        h = self.max_1(h)\n        \n        h = self.conv_2d_2(h)\n        h = F.leaky_relu(h,0.01)\n        h = self.max_1(h)\n        \n        h = self.flatten(h)\n        \n        h = self.linear_1(h)\n        h = F.leaky_relu(h,0.01)\n        h = self.batch_norm_1(h)\n        \n        h = self.linear_2(h)\n        h = F.leaky_relu(h,0.01)\n        h = self.batch_norm_2(h)\n        \n        h = self.linear_3(h)\n\n        \n#         h = self.softmax(h)\n        return h\nmodel = ClassificationImage().to(device)\nloss_fn = torch.nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ntotal_para = sum(p.numel() for p in model.parameters())\nprint(total_para)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T15:33:00.812480Z","iopub.execute_input":"2023-04-06T15:33:00.813397Z","iopub.status.idle":"2023-04-06T15:33:00.831238Z","shell.execute_reply.started":"2023-04-06T15:33:00.813343Z","shell.execute_reply":"2023-04-06T15:33:00.830165Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"35006\nClassificationImage(\n  (conv_2d_1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (max_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv_2d_2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_1): Linear(in_features=1024, out_features=32, bias=True)\n  (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (linear_2): Linear(in_features=32, out_features=16, bias=True)\n  (batch_norm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (linear_3): Linear(in_features=16, out_features=10, bias=True)\n  (batch_norm_3): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (softmax): Softmax(dim=1)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"epochs_time = []\n\nepochs_loss = []\nepochs_acc_accuracy = []\nepochs_acc_precision = []\nepochs_acc_recall = []\nepochs_acc_f1 = []\n\nepochs_test_loss = []\nepochs_test_acc_accuracy = []\nepochs_test_acc_precision = []\nepochs_test_acc_recall = []\nepochs_test_acc_f1 = []\n\nloss_list, test_loss_list = [], []\n\nacc_f1, test_acc_f1 = [], []\nacc_accuracy, test_acc_accuracy = [], []\nacc_precision, test_acc_precision = [], []\nacc_recall, test_acc_recall = [], []\n\nfor i in range(1000):\n    model.train()\n    for x,y in train_iter:\n        x = x.float().to(device)\n        y = y.to(device)\n        \n        pred = model(x)\n        optimizer.zero_grad() \n        loss = loss_fn(pred, y)\n        \n        loss_list.append(loss.item())\n        loss.backward()\n        optimizer.step()\n\n        ground_truth = torch.argmax( torch.clone(y), dim=1).cpu().detach().numpy().copy()\n        pred_ind = torch.argmax( torch.clone(pred), dim=1).cpu().detach().numpy().copy()\n\n        accuracy = accuracy_score(ground_truth, pred_ind)\n        acc_accuracy.append(accuracy)\n        \n        precision = precision_score(ground_truth, pred_ind, average= \"micro\")\n        acc_precision.append(precision)\n        \n        recall = recall_score(ground_truth, pred_ind,  average='micro')\n        acc_recall.append(recall)\n        \n        f1 = f1_score(ground_truth, pred_ind, average=\"micro\")\n        acc_f1.append(f1)\n\n    model.eval()\n    for x_test,y_test in test_iter:\n        x_test = x_test.float().to(device)\n        y_test = y_test.to(device)\n  \n        pred_test = model(x_test)\n        loss_test = loss_fn(pred_test, y_test)\n        test_loss_list.append(loss_test.item())\n        \n        ground_truth_test = torch.argmax(torch.clone(y_test), dim=1).cpu().detach().numpy().copy()\n        pred_ind_test = torch.argmax(torch.clone(pred_test), dim=1).cpu().detach().numpy().copy()\n    \n        test_accuracy = accuracy_score(ground_truth_test, pred_ind_test)\n        test_acc_accuracy.append(test_accuracy)\n        \n        test_precision = precision_score(ground_truth_test, pred_ind_test, average= \"micro\")\n        test_acc_precision.append(test_precision)\n        \n        test_recall = recall_score(ground_truth_test, pred_ind_test,  average='micro')\n        test_acc_recall.append(test_recall)\n    \n        test_f1 = f1_score(ground_truth_test, pred_ind_test, average=\"micro\")\n        test_acc_f1.append(test_f1)\n    epochs_time.append(i)\n    \n    total_loss = sum(loss_list)/len(loss_list)\n    test_total_loss = sum(test_loss_list)/len(test_loss_list)\n    epochs_loss.append(total_loss)\n    epochs_test_loss.append(test_total_loss)\n    \n    total_accuracy = sum(acc_accuracy)/len(acc_accuracy)\n    test_total_accuracy = sum(test_acc_accuracy)/len(test_acc_accuracy)\n    epochs_acc_accuracy.append(total_accuracy)\n    epochs_test_acc_accuracy.append(test_total_accuracy)\n    \n    \n    total_precision = sum(acc_precision)/len(acc_precision)\n    test_total_precision = sum(test_acc_precision)/len(test_acc_precision)\n    epochs_acc_precision.append(total_precision)\n    epochs_test_acc_precision.append(test_total_precision)\n    \n    total_recall = sum(acc_recall)/len(acc_recall)\n    test_total_recall = sum(test_acc_recall)/len(test_acc_recall)\n    epochs_acc_recall.append(total_recall)\n    epochs_test_acc_recall.append(test_total_recall)\n    \n    total_f1 = sum(acc_f1)/len(acc_f1)\n    test_total_f1 = sum(test_acc_f1) / len(test_acc_f1)\n    epochs_acc_f1.append(total_f1)\n    epochs_test_acc_f1.append(test_total_f1)\n    if i % 10 == 0:\n        print(f\"Epochs: {i}, loss: {total_loss}, test loss: {test_total_loss}\")\n        print(f\"Accuracy acc: {total_accuracy}, test accuracy acc: {test_total_accuracy}\")\n        print(f\"Accuracy precision: {total_precision}, test accuracy precision: {test_total_precision}\")\n        print(f\"Accuracy recall: {total_recall}, test accuracy recall: {test_total_recall}\")\n        print(f\"Accuracy f1: {total_f1}, test accuracy f1: {test_total_f1}\")\n        print(f\"=======================================================================\")","metadata":{"execution":{"iopub.status.busy":"2023-04-06T15:33:03.859885Z","iopub.execute_input":"2023-04-06T15:33:03.860253Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epochs: 0, loss: 1.981846021700509, test loss: 1.9306379556655884\nAccuracy acc: 0.2786540743670886, test accuracy acc: 0.3671875\nAccuracy precision: 0.2786540743670886, test accuracy precision: 0.3671875\nAccuracy recall: 0.2786540743670886, test accuracy recall: 0.3671875\nAccuracy f1: 0.2786540743670886, test accuracy f1: 0.3671875\n=======================================================================\nEpochs: 10, loss: 1.5271020389673213, test loss: 1.725640058517456\nAccuracy acc: 0.4454046497410817, test accuracy acc: 0.4069602272727273\nAccuracy precision: 0.4454046497410817, test accuracy precision: 0.4069602272727273\nAccuracy recall: 0.4454046497410817, test accuracy recall: 0.4069602272727273\nAccuracy f1: 0.4454046497410817, test accuracy f1: 0.4069602272727273\n=======================================================================\nEpochs: 20, loss: 1.4103698846875652, test loss: 1.6529547146388464\nAccuracy acc: 0.4900848591018686, test accuracy acc: 0.4334077380952381\nAccuracy precision: 0.4900848591018686, test accuracy precision: 0.4334077380952381\nAccuracy recall: 0.4900848591018686, test accuracy recall: 0.4334077380952381\nAccuracy f1: 0.4900848591018686, test accuracy f1: 0.4334077380952381\n=======================================================================\nEpochs: 30, loss: 1.3403354412976844, test loss: 1.6605413959872337\nAccuracy acc: 0.5164950936606778, test accuracy acc: 0.4369959677419355\nAccuracy precision: 0.5164950936606778, test accuracy precision: 0.4369959677419355\nAccuracy recall: 0.5164950936606778, test accuracy recall: 0.4369959677419355\nAccuracy f1: 0.5164950936606778, test accuracy f1: 0.4369959677419355\n=======================================================================\nEpochs: 40, loss: 1.289525565540912, test loss: 1.6867939437307962\nAccuracy acc: 0.5352653934470516, test accuracy acc: 0.4405487804878049\nAccuracy precision: 0.5352653934470516, test accuracy precision: 0.4405487804878049\nAccuracy recall: 0.5352653934470516, test accuracy recall: 0.4405487804878049\nAccuracy f1: 0.5352653934470516, test accuracy f1: 0.4405487804878049\n=======================================================================\nEpochs: 50, loss: 1.2475147969552913, test loss: 1.7007600256040984\nAccuracy acc: 0.5505068720526185, test accuracy acc: 0.4427083333333333\nAccuracy precision: 0.5505068720526185, test accuracy precision: 0.4427083333333333\nAccuracy recall: 0.5505068720526185, test accuracy recall: 0.4427083333333333\nAccuracy f1: 0.5505068720526185, test accuracy f1: 0.4427083333333333\n=======================================================================\nEpochs: 60, loss: 1.212460893037582, test loss: 1.7243552012521712\nAccuracy acc: 0.5632878968665699, test accuracy acc: 0.444672131147541\nAccuracy precision: 0.5632878968665699, test accuracy precision: 0.444672131147541\nAccuracy recall: 0.5632878968665699, test accuracy recall: 0.444672131147541\nAccuracy f1: 0.5632878968665699, test accuracy f1: 0.444672131147541\n=======================================================================\nEpochs: 70, loss: 1.1824609896018343, test loss: 1.7515198831826868\nAccuracy acc: 0.5741560717151007, test accuracy acc: 0.4431117957746479\nAccuracy precision: 0.5741560717151007, test accuracy precision: 0.4431117957746479\nAccuracy recall: 0.5741560717151007, test accuracy recall: 0.4431117957746479\nAccuracy f1: 0.5741560717151007, test accuracy f1: 0.4431117957746479\n=======================================================================\nEpochs: 80, loss: 1.1555062607035522, test loss: 1.7847161204726607\nAccuracy acc: 0.583814060302391, test accuracy acc: 0.44145447530864196\nAccuracy precision: 0.583814060302391, test accuracy precision: 0.44145447530864196\nAccuracy recall: 0.583814060302391, test accuracy recall: 0.44145447530864196\nAccuracy f1: 0.583814060302391, test accuracy f1: 0.44145447530864196\n=======================================================================\nEpochs: 90, loss: 1.1307992105491305, test loss: 1.8167888947895594\nAccuracy acc: 0.5927072828279316, test accuracy acc: 0.4405048076923077\nAccuracy precision: 0.5927072828279316, test accuracy precision: 0.4405048076923077\nAccuracy recall: 0.5927072828279316, test accuracy recall: 0.4405048076923077\nAccuracy f1: 0.5927072828279316, test accuracy f1: 0.4405048076923077\n=======================================================================\nEpochs: 100, loss: 1.1086423383916675, test loss: 1.8452610591850658\nAccuracy acc: 0.6006066216787818, test accuracy acc: 0.43742264851485146\nAccuracy precision: 0.6006066216787818, test accuracy precision: 0.43742264851485146\nAccuracy recall: 0.6006066216787818, test accuracy recall: 0.43742264851485146\nAccuracy f1: 0.6006066216787818, test accuracy f1: 0.43742264851485146\n=======================================================================\nEpochs: 110, loss: 1.0883880331152207, test loss: 1.8714785382554338\nAccuracy acc: 0.6077938351721975, test accuracy acc: 0.43552927927927926\nAccuracy precision: 0.6077938351721975, test accuracy precision: 0.43552927927927926\nAccuracy recall: 0.6077938351721975, test accuracy recall: 0.43552927927927926\nAccuracy f1: 0.6077938351721975, test accuracy f1: 0.43552927927927926\n=======================================================================\nEpochs: 120, loss: 1.0692306876818571, test loss: 1.905454738080994\nAccuracy acc: 0.6146301914426195, test accuracy acc: 0.43304493801652894\nAccuracy precision: 0.6146301914426195, test accuracy precision: 0.43304493801652894\nAccuracy recall: 0.6146301914426195, test accuracy recall: 0.43304493801652894\nAccuracy f1: 0.6146301914426195, test accuracy f1: 0.43304493801652894\n=======================================================================\nEpochs: 130, loss: 1.0517983127935047, test loss: 1.9359245245693295\nAccuracy acc: 0.6208710533626437, test accuracy acc: 0.43225190839694655\nAccuracy precision: 0.6208710533626437, test accuracy precision: 0.43225190839694655\nAccuracy recall: 0.6208710533626437, test accuracy recall: 0.43225190839694655\nAccuracy f1: 0.6208710533626437, test accuracy f1: 0.43225190839694655\n=======================================================================\nEpochs: 140, loss: 1.035564285761279, test loss: 1.9625419792554057\nAccuracy acc: 0.6266489081156298, test accuracy acc: 0.4318484042553192\nAccuracy precision: 0.6266489081156298, test accuracy precision: 0.4318484042553192\nAccuracy recall: 0.6266489081156298, test accuracy recall: 0.4318484042553192\nAccuracy f1: 0.6266489081156298, test accuracy f1: 0.4318484042553192\n=======================================================================\nEpochs: 150, loss: 1.0206651813386922, test loss: 1.9941376187153999\nAccuracy acc: 0.631919848688071, test accuracy acc: 0.431601821192053\nAccuracy precision: 0.631919848688071, test accuracy precision: 0.431601821192053\nAccuracy recall: 0.631919848688071, test accuracy recall: 0.431601821192053\nAccuracy f1: 0.631919848688071, test accuracy f1: 0.431601821192053\n=======================================================================\nEpochs: 160, loss: 1.0063329833897323, test loss: 2.0291927542005266\nAccuracy acc: 0.6369513928473151, test accuracy acc: 0.429881599378882\nAccuracy precision: 0.6369513928473151, test accuracy precision: 0.429881599378882\nAccuracy recall: 0.6369513928473151, test accuracy recall: 0.429881599378882\nAccuracy f1: 0.6369513928473151, test accuracy f1: 0.429881599378882\n=======================================================================\nEpochs: 170, loss: 0.9930703141138832, test loss: 2.0557330742216946\nAccuracy acc: 0.6416568641738841, test accuracy acc: 0.4295504385964912\nAccuracy precision: 0.6416568641738841, test accuracy precision: 0.4295504385964912\nAccuracy recall: 0.6416568641738841, test accuracy recall: 0.4295504385964912\nAccuracy f1: 0.6416568641738841, test accuracy f1: 0.4295504385964912\n=======================================================================\nEpochs: 180, loss: 0.9805940729403414, test loss: 2.0866578244372627\nAccuracy acc: 0.6460680434383523, test accuracy acc: 0.4293853591160221\nAccuracy precision: 0.6460680434383523, test accuracy precision: 0.4293853591160221\nAccuracy recall: 0.6460680434383523, test accuracy recall: 0.4293853591160221\nAccuracy f1: 0.6460680434383523, test accuracy f1: 0.4293853591160221\n=======================================================================\nEpochs: 190, loss: 0.9686457019407159, test loss: 2.11601182677983\nAccuracy acc: 0.6502950980432766, test accuracy acc: 0.429523887434555\nAccuracy precision: 0.6502950980432766, test accuracy precision: 0.429523887434555\nAccuracy recall: 0.6502950980432766, test accuracy recall: 0.429523887434555\nAccuracy f1: 0.6502950980432766, test accuracy f1: 0.429523887434555\n=======================================================================\nEpochs: 200, loss: 0.9571804687961672, test loss: 2.14570934974139\nAccuracy acc: 0.6543257801183954, test accuracy acc: 0.43062033582089554\nAccuracy precision: 0.6543257801183954, test accuracy precision: 0.43062033582089554\nAccuracy recall: 0.6543257801183954, test accuracy recall: 0.43062033582089554\nAccuracy f1: 0.6543257801183954, test accuracy f1: 0.43062033582089554\n=======================================================================\nEpochs: 210, loss: 0.946537388151298, test loss: 2.1785984265295815\nAccuracy acc: 0.6580895862079309, test accuracy acc: 0.43109449052132703\nAccuracy precision: 0.6580895862079309, test accuracy precision: 0.43109449052132703\nAccuracy recall: 0.6580895862079309, test accuracy recall: 0.43109449052132703\nAccuracy f1: 0.6580895862079309, test accuracy f1: 0.43109449052132703\n=======================================================================\nEpochs: 220, loss: 0.9362513462841269, test loss: 2.2055177267859962\nAccuracy acc: 0.6617311451114039, test accuracy acc: 0.4313489819004525\nAccuracy precision: 0.6617311451114039, test accuracy precision: 0.4313489819004525\nAccuracy recall: 0.6617311451114039, test accuracy recall: 0.4313489819004525\nAccuracy f1: 0.6617311451114039, test accuracy f1: 0.4313489819004525\n=======================================================================\nEpochs: 230, loss: 0.926522545810856, test loss: 2.2343939599536715\nAccuracy acc: 0.6651895951490493, test accuracy acc: 0.4317167207792208\nAccuracy precision: 0.6651895951490493, test accuracy precision: 0.4317167207792208\nAccuracy recall: 0.6651895951490493, test accuracy recall: 0.4317167207792208\nAccuracy f1: 0.6651895951490493, test accuracy f1: 0.4317167207792208\n=======================================================================\nEpochs: 240, loss: 0.9175019904378645, test loss: 2.2606030747108936\nAccuracy acc: 0.6683966340472188, test accuracy acc: 0.4322808609958506\nAccuracy precision: 0.6683966340472188, test accuracy precision: 0.4322808609958506\nAccuracy recall: 0.6683966340472188, test accuracy recall: 0.4322808609958506\nAccuracy f1: 0.6683966340472188, test accuracy f1: 0.4322808609958506\n=======================================================================\nEpochs: 250, loss: 0.908489119670498, test loss: 2.2855979037949763\nAccuracy acc: 0.6716169341620858, test accuracy acc: 0.43236429282868527\nAccuracy precision: 0.6716169341620858, test accuracy precision: 0.43236429282868527\nAccuracy recall: 0.6716169341620858, test accuracy recall: 0.43236429282868527\nAccuracy f1: 0.6716169341620858, test accuracy f1: 0.43236429282868527\n=======================================================================\nEpochs: 260, loss: 0.9000087208033036, test loss: 2.315240130113916\nAccuracy acc: 0.6746132018708473, test accuracy acc: 0.4329501915708812\nAccuracy precision: 0.6746132018708473, test accuracy precision: 0.4329501915708812\nAccuracy recall: 0.6746132018708473, test accuracy recall: 0.4329501915708812\nAccuracy f1: 0.6746132018708473, test accuracy f1: 0.4329501915708812\n=======================================================================\nEpochs: 270, loss: 0.891821067653058, test loss: 2.341237230054567\nAccuracy acc: 0.6775020143397636, test accuracy acc: 0.4330027675276753\nAccuracy precision: 0.6775020143397636, test accuracy precision: 0.4330027675276753\nAccuracy recall: 0.6775020143397636, test accuracy recall: 0.4330027675276753\nAccuracy f1: 0.6775020143397636, test accuracy f1: 0.4330027675276753\n=======================================================================\nEpochs: 280, loss: 0.8839657562391957, test loss: 2.3676734055488557\nAccuracy acc: 0.6802972191878013, test accuracy acc: 0.43352424377224197\nAccuracy precision: 0.6802972191878013, test accuracy precision: 0.43352424377224197\nAccuracy recall: 0.6802972191878013, test accuracy recall: 0.43352424377224197\nAccuracy f1: 0.6802972191878013, test accuracy f1: 0.43352424377224197\n=======================================================================\nEpochs: 290, loss: 0.8763818370056078, test loss: 2.3942829538456762\nAccuracy acc: 0.6830081268487103, test accuracy acc: 0.4339830326460481\nAccuracy precision: 0.6830081268487103, test accuracy precision: 0.4339830326460481\nAccuracy recall: 0.6830081268487103, test accuracy recall: 0.4339830326460481\nAccuracy f1: 0.6830081268487103, test accuracy f1: 0.4339830326460481\n=======================================================================\nEpochs: 300, loss: 0.8691428864597297, test loss: 2.419048467743832\nAccuracy acc: 0.6855666699083225, test accuracy acc: 0.43480066445182725\nAccuracy precision: 0.6855666699083225, test accuracy precision: 0.43480066445182725\nAccuracy recall: 0.6855666699083225, test accuracy recall: 0.43480066445182725\nAccuracy f1: 0.6855666699083225, test accuracy f1: 0.43480066445182725\n=======================================================================\nEpochs: 310, loss: 0.8621179842168709, test loss: 2.443542402273589\nAccuracy acc: 0.6880415233627742, test accuracy acc: 0.4348623392282958\nAccuracy precision: 0.6880415233627742, test accuracy precision: 0.4348623392282958\nAccuracy recall: 0.6880415233627742, test accuracy recall: 0.4348623392282958\nAccuracy f1: 0.6880415233627742, test accuracy f1: 0.4348623392282958\n=======================================================================\nEpochs: 320, loss: 0.8554242610428404, test loss: 2.4663284976163014\nAccuracy acc: 0.6904112414478094, test accuracy acc: 0.4349688473520249\nAccuracy precision: 0.6904112414478094, test accuracy precision: 0.4349688473520249\nAccuracy recall: 0.6904112414478094, test accuracy recall: 0.4349688473520249\nAccuracy f1: 0.6904112414478094, test accuracy f1: 0.4349688473520249\n=======================================================================\nEpochs: 330, loss: 0.848890025570833, test loss: 2.4924531940967296\nAccuracy acc: 0.69271776944147, test accuracy acc: 0.4349273036253776\nAccuracy precision: 0.69271776944147, test accuracy precision: 0.4349273036253776\nAccuracy recall: 0.69271776944147, test accuracy recall: 0.4349273036253776\nAccuracy f1: 0.69271776944147, test accuracy f1: 0.4349273036253776\n=======================================================================\nEpochs: 340, loss: 0.84272020261791, test loss: 2.5135536165880668\nAccuracy acc: 0.6948911197520323, test accuracy acc: 0.4350943914956012\nAccuracy precision: 0.6948911197520323, test accuracy precision: 0.4350943914956012\nAccuracy recall: 0.6948911197520323, test accuracy recall: 0.4350943914956012\nAccuracy f1: 0.6948911197520323, test accuracy f1: 0.4350943914956012\n=======================================================================\nEpochs: 350, loss: 0.8365479965630669, test loss: 2.5357669119821313\nAccuracy acc: 0.6970753768617692, test accuracy acc: 0.43469551282051283\nAccuracy precision: 0.6970753768617692, test accuracy precision: 0.43469551282051283\nAccuracy recall: 0.6970753768617692, test accuracy recall: 0.43469551282051283\nAccuracy f1: 0.6970753768617692, test accuracy f1: 0.43469551282051283\n=======================================================================\nEpochs: 360, loss: 0.8308434003840038, test loss: 2.5587515560213547\nAccuracy acc: 0.6990793829333076, test accuracy acc: 0.43460006925207756\nAccuracy precision: 0.6990793829333076, test accuracy precision: 0.43460006925207756\nAccuracy recall: 0.6990793829333076, test accuracy recall: 0.43460006925207756\nAccuracy f1: 0.6990793829333076, test accuracy f1: 0.43460006925207756\n=======================================================================\nEpochs: 370, loss: 0.8251689228063767, test loss: 2.580478805737354\nAccuracy acc: 0.7010974391398547, test accuracy acc: 0.43461506064690025\nAccuracy precision: 0.7010974391398547, test accuracy precision: 0.43461506064690025\nAccuracy recall: 0.7010974391398547, test accuracy recall: 0.43461506064690025\nAccuracy f1: 0.7010974391398547, test accuracy f1: 0.43461506064690025\n=======================================================================\nEpochs: 380, loss: 0.8196421885669435, test loss: 2.6016898030058293\nAccuracy acc: 0.7030437576829796, test accuracy acc: 0.434690780839895\nAccuracy precision: 0.7030437576829796, test accuracy precision: 0.434690780839895\nAccuracy recall: 0.7030437576829796, test accuracy recall: 0.434690780839895\nAccuracy f1: 0.7030437576829796, test accuracy f1: 0.434690780839895\n=======================================================================\nEpochs: 390, loss: 0.8142272127929304, test loss: 2.622884643657128\nAccuracy acc: 0.7049601370026547, test accuracy acc: 0.43484255115089515\nAccuracy precision: 0.7049601370026547, test accuracy precision: 0.43484255115089515\nAccuracy recall: 0.7049601370026547, test accuracy recall: 0.43484255115089515\nAccuracy f1: 0.7049601370026547, test accuracy f1: 0.43484255115089515\n=======================================================================\nEpochs: 400, loss: 0.809047372076652, test loss: 2.6461473034504346\nAccuracy acc: 0.706793636951924, test accuracy acc: 0.43449968827930174\nAccuracy precision: 0.706793636951924, test accuracy precision: 0.43449968827930174\nAccuracy recall: 0.706793636951924, test accuracy recall: 0.43449968827930174\nAccuracy f1: 0.706793636951924, test accuracy f1: 0.43449968827930174\n=======================================================================\nEpochs: 410, loss: 0.8039932871961539, test loss: 2.6688756281442014\nAccuracy acc: 0.7085735263828575, test accuracy acc: 0.4345726885644769\nAccuracy precision: 0.7085735263828575, test accuracy precision: 0.4345726885644769\nAccuracy recall: 0.7085735263828575, test accuracy recall: 0.4345726885644769\nAccuracy f1: 0.7085735263828575, test accuracy f1: 0.4345726885644769\n=======================================================================\nEpochs: 420, loss: 0.7991070313248511, test loss: 2.6903904458406136\nAccuracy acc: 0.7103003369388737, test accuracy acc: 0.43458654988123513\nAccuracy precision: 0.7103003369388737, test accuracy precision: 0.43458654988123513\nAccuracy recall: 0.7103003369388737, test accuracy recall: 0.43458654988123513\nAccuracy f1: 0.7103003369388737, test accuracy f1: 0.43458654988123513\n=======================================================================\nEpochs: 430, loss: 0.7943662569820031, test loss: 2.714420273243136\nAccuracy acc: 0.7119788530867279, test accuracy acc: 0.4346178944315545\nAccuracy precision: 0.7119788530867279, test accuracy precision: 0.4346178944315545\nAccuracy recall: 0.7119788530867279, test accuracy recall: 0.4346178944315545\nAccuracy f1: 0.7119788530867279, test accuracy f1: 0.4346178944315545\n=======================================================================\nEpochs: 440, loss: 0.7897258615330558, test loss: 2.736321539565279\nAccuracy acc: 0.7135982887353254, test accuracy acc: 0.4347009637188209\nAccuracy precision: 0.7135982887353254, test accuracy precision: 0.4347009637188209\nAccuracy recall: 0.7135982887353254, test accuracy recall: 0.4347009637188209\nAccuracy f1: 0.7135982887353254, test accuracy f1: 0.4347009637188209\n=======================================================================\nEpochs: 450, loss: 0.7851986818864793, test loss: 2.7601778861424346\nAccuracy acc: 0.7151820892040192, test accuracy acc: 0.4347630266075388\nAccuracy precision: 0.7151820892040192, test accuracy precision: 0.4347630266075388\nAccuracy recall: 0.7151820892040192, test accuracy recall: 0.4347630266075388\nAccuracy f1: 0.7151820892040192, test accuracy f1: 0.4347630266075388\n=======================================================================\nEpochs: 460, loss: 0.7808821532587763, test loss: 2.7810212707312663\nAccuracy acc: 0.7167029701186194, test accuracy acc: 0.4345851409978308\nAccuracy precision: 0.7167029701186194, test accuracy precision: 0.4345851409978308\nAccuracy recall: 0.7167029701186194, test accuracy recall: 0.4345851409978308\nAccuracy f1: 0.7167029701186194, test accuracy f1: 0.4345851409978308\n=======================================================================\nEpochs: 470, loss: 0.7765864586352136, test loss: 2.8015566444194495\nAccuracy acc: 0.7182078239639603, test accuracy acc: 0.43477972399150744\nAccuracy precision: 0.7182078239639603, test accuracy precision: 0.43477972399150744\nAccuracy recall: 0.7182078239639603, test accuracy recall: 0.43477972399150744\nAccuracy f1: 0.7182078239639603, test accuracy f1: 0.43477972399150744\n=======================================================================\nEpochs: 480, loss: 0.7724093211819025, test loss: 2.8219458115076077\nAccuracy acc: 0.7196731842449802, test accuracy acc: 0.434706340956341\nAccuracy precision: 0.7196731842449802, test accuracy precision: 0.434706340956341\nAccuracy recall: 0.7196731842449802, test accuracy recall: 0.434706340956341\nAccuracy f1: 0.7196731842449802, test accuracy f1: 0.434706340956341\n=======================================================================\nEpochs: 490, loss: 0.7683256598110096, test loss: 2.8427853623135757\nAccuracy acc: 0.7211136996796772, test accuracy acc: 0.43450865580448067\nAccuracy precision: 0.7211136996796772, test accuracy precision: 0.43450865580448067\nAccuracy recall: 0.7211136996796772, test accuracy recall: 0.43450865580448067\nAccuracy f1: 0.7211136996796772, test accuracy f1: 0.43450865580448067\n=======================================================================\nEpochs: 500, loss: 0.7643856947855264, test loss: 2.8645264607465672\nAccuracy acc: 0.7225000651387099, test accuracy acc: 0.4344436127744511\nAccuracy precision: 0.7225000651387099, test accuracy precision: 0.4344436127744511\nAccuracy recall: 0.7225000651387099, test accuracy recall: 0.4344436127744511\nAccuracy f1: 0.7225000651387099, test accuracy f1: 0.4344436127744511\n=======================================================================\nEpochs: 510, loss: 0.7605856002652798, test loss: 2.885605734155136\nAccuracy acc: 0.7238293151923506, test accuracy acc: 0.4341059197651663\nAccuracy precision: 0.7238293151923506, test accuracy precision: 0.4341059197651663\nAccuracy recall: 0.7238293151923506, test accuracy recall: 0.4341059197651663\nAccuracy f1: 0.7238293151923506, test accuracy f1: 0.4341059197651663\n=======================================================================\nEpochs: 520, loss: 0.7567978759721566, test loss: 2.9062957530287084\nAccuracy acc: 0.725176867453048, test accuracy acc: 0.43396113243761997\nAccuracy precision: 0.725176867453048, test accuracy precision: 0.43396113243761997\nAccuracy recall: 0.725176867453048, test accuracy recall: 0.43396113243761997\nAccuracy f1: 0.725176867453048, test accuracy f1: 0.43396113243761997\n=======================================================================\nEpochs: 530, loss: 0.753073829670777, test loss: 2.926180818884611\nAccuracy acc: 0.7264845128161577, test accuracy acc: 0.4339395009416196\nAccuracy precision: 0.7264845128161577, test accuracy precision: 0.4339395009416196\nAccuracy recall: 0.7264845128161577, test accuracy recall: 0.4339395009416196\nAccuracy f1: 0.7264845128161577, test accuracy f1: 0.4339395009416196\n=======================================================================\nEpochs: 540, loss: 0.7495040353007124, test loss: 2.948683618397457\nAccuracy acc: 0.7277482491840006, test accuracy acc: 0.4336442929759704\nAccuracy precision: 0.7277482491840006, test accuracy precision: 0.4336442929759704\nAccuracy recall: 0.7277482491840006, test accuracy recall: 0.4336442929759704\nAccuracy f1: 0.7277482491840006, test accuracy f1: 0.4336442929759704\n=======================================================================\nEpochs: 550, loss: 0.7459860915871118, test loss: 2.970709957790894\nAccuracy acc: 0.7289876073996646, test accuracy acc: 0.43348740925589835\nAccuracy precision: 0.7289876073996646, test accuracy precision: 0.43348740925589835\nAccuracy recall: 0.7289876073996646, test accuracy recall: 0.43348740925589835\nAccuracy f1: 0.7289876073996646, test accuracy f1: 0.43348740925589835\n=======================================================================\nEpochs: 560, loss: 0.7427098359312052, test loss: 2.9918380076872473\nAccuracy acc: 0.7301546652536158, test accuracy acc: 0.4332943404634581\nAccuracy precision: 0.7301546652536158, test accuracy precision: 0.4332943404634581\nAccuracy recall: 0.7301546652536158, test accuracy recall: 0.4332943404634581\nAccuracy f1: 0.7301546652536158, test accuracy f1: 0.4332943404634581\n=======================================================================\nEpochs: 570, loss: 0.7393032900835284, test loss: 3.0110364624163735\nAccuracy acc: 0.731355967282028, test accuracy acc: 0.43321749124343256\nAccuracy precision: 0.731355967282028, test accuracy precision: 0.43321749124343256\nAccuracy recall: 0.731355967282028, test accuracy recall: 0.43321749124343256\nAccuracy f1: 0.731355967282028, test accuracy f1: 0.43321749124343256\n=======================================================================\nEpochs: 580, loss: 0.7361897727780027, test loss: 3.0299646370177014\nAccuracy acc: 0.7324582575328439, test accuracy acc: 0.4330088209982788\nAccuracy precision: 0.7324582575328439, test accuracy precision: 0.4330088209982788\nAccuracy recall: 0.7324582575328439, test accuracy recall: 0.4330088209982788\nAccuracy f1: 0.7324582575328439, test accuracy f1: 0.4330088209982788\n=======================================================================\nEpochs: 590, loss: 0.7328496547780291, test loss: 3.050850711296456\nAccuracy acc: 0.7336422173183191, test accuracy acc: 0.4326750211505922\nAccuracy precision: 0.7336422173183191, test accuracy precision: 0.4326750211505922\nAccuracy recall: 0.7336422173183191, test accuracy recall: 0.4326750211505922\nAccuracy f1: 0.7336422173183191, test accuracy f1: 0.4326750211505922\n=======================================================================\nEpochs: 600, loss: 0.7298573682566004, test loss: 3.0702794645471303\nAccuracy acc: 0.734699650108469, test accuracy acc: 0.4323913269550749\nAccuracy precision: 0.734699650108469, test accuracy precision: 0.4323913269550749\nAccuracy recall: 0.734699650108469, test accuracy recall: 0.4323913269550749\nAccuracy f1: 0.734699650108469, test accuracy f1: 0.4323913269550749\n=======================================================================\nEpochs: 610, loss: 0.7267898988102875, test loss: 3.089204076073954\nAccuracy acc: 0.7357838526020841, test accuracy acc: 0.4322192103109656\nAccuracy precision: 0.7357838526020841, test accuracy precision: 0.4322192103109656\nAccuracy recall: 0.7357838526020841, test accuracy recall: 0.4322192103109656\nAccuracy f1: 0.7357838526020841, test accuracy f1: 0.4322192103109656\n=======================================================================\nEpochs: 620, loss: 0.7236763396019208, test loss: 3.1080174941371603\nAccuracy acc: 0.736891341930125, test accuracy acc: 0.4321407004830918\nAccuracy precision: 0.736891341930125, test accuracy precision: 0.4321407004830918\nAccuracy recall: 0.736891341930125, test accuracy recall: 0.4321407004830918\nAccuracy f1: 0.736891341930125, test accuracy f1: 0.4321407004830918\n=======================================================================\nEpochs: 630, loss: 0.720789531681176, test loss: 3.12674941993931\nAccuracy acc: 0.7379113438208389, test accuracy acc: 0.4319903922345483\nAccuracy precision: 0.7379113438208389, test accuracy precision: 0.4319903922345483\nAccuracy recall: 0.7379113438208389, test accuracy recall: 0.4319903922345483\nAccuracy f1: 0.7379113438208389, test accuracy f1: 0.4319903922345483\n=======================================================================\nEpochs: 640, loss: 0.7177810612758821, test loss: 3.1493961089486824\nAccuracy acc: 0.7389760809973538, test accuracy acc: 0.4317716458658346\nAccuracy precision: 0.7389760809973538, test accuracy precision: 0.4317716458658346\nAccuracy recall: 0.7389760809973538, test accuracy recall: 0.4317716458658346\nAccuracy f1: 0.7389760809973538, test accuracy f1: 0.4317716458658346\n=======================================================================\nEpochs: 650, loss: 0.7148962681921278, test loss: 3.169478981542514\nAccuracy acc: 0.7399972079225728, test accuracy acc: 0.4318236367127496\nAccuracy precision: 0.7399972079225728, test accuracy precision: 0.4318236367127496\nAccuracy recall: 0.7399972079225728, test accuracy recall: 0.4318236367127496\nAccuracy f1: 0.7399972079225728, test accuracy f1: 0.4318236367127496\n=======================================================================\nEpochs: 660, loss: 0.7121246016174615, test loss: 3.1919886178580787\nAccuracy acc: 0.7409712431059576, test accuracy acc: 0.43161403177004537\nAccuracy precision: 0.7409712431059576, test accuracy precision: 0.43161403177004537\nAccuracy recall: 0.7409712431059576, test accuracy recall: 0.43161403177004537\nAccuracy f1: 0.7409712431059576, test accuracy f1: 0.43161403177004537\n=======================================================================\nEpochs: 670, loss: 0.709313030821684, test loss: 3.2150462611776884\nAccuracy acc: 0.7419679397012772, test accuracy acc: 0.43149217585692995\nAccuracy precision: 0.7419679397012772, test accuracy precision: 0.43149217585692995\nAccuracy recall: 0.7419679397012772, test accuracy recall: 0.43149217585692995\nAccuracy f1: 0.7419679397012772, test accuracy f1: 0.43149217585692995\n=======================================================================\nEpochs: 680, loss: 0.7065277517586407, test loss: 3.2351876344974873\nAccuracy acc: 0.7429499952949868, test accuracy acc: 0.43125917767988253\nAccuracy precision: 0.7429499952949868, test accuracy precision: 0.43125917767988253\nAccuracy recall: 0.7429499952949868, test accuracy recall: 0.43125917767988253\nAccuracy f1: 0.7429499952949868, test accuracy f1: 0.43125917767988253\n=======================================================================\nEpochs: 690, loss: 0.7039082395739767, test loss: 3.2558671945083333\nAccuracy acc: 0.7438792613896572, test accuracy acc: 0.4313268813314038\nAccuracy precision: 0.7438792613896572, test accuracy precision: 0.4313268813314038\nAccuracy recall: 0.7438792613896572, test accuracy recall: 0.4313268813314038\nAccuracy f1: 0.7438792613896572, test accuracy f1: 0.4313268813314038\n=======================================================================\nEpochs: 700, loss: 0.7013521121145554, test loss: 3.2758572945071016\nAccuracy acc: 0.7447916078861121, test accuracy acc: 0.43101373038516405\nAccuracy precision: 0.7447916078861121, test accuracy precision: 0.43101373038516405\nAccuracy recall: 0.7447916078861121, test accuracy recall: 0.43101373038516405\nAccuracy f1: 0.7447916078861121, test accuracy f1: 0.43101373038516405\n=======================================================================\nEpochs: 710, loss: 0.698747602145412, test loss: 3.2946526072028655\nAccuracy acc: 0.7457163662896794, test accuracy acc: 0.4309731012658228\nAccuracy precision: 0.7457163662896794, test accuracy precision: 0.4309731012658228\nAccuracy recall: 0.7457163662896794, test accuracy recall: 0.4309731012658228\nAccuracy f1: 0.7457163662896794, test accuracy f1: 0.4309731012658228\n=======================================================================\nEpochs: 720, loss: 0.6961901157141615, test loss: 3.3138003160817946\nAccuracy acc: 0.7466245937318948, test accuracy acc: 0.43068437933425796\nAccuracy precision: 0.7466245937318948, test accuracy precision: 0.43068437933425796\nAccuracy recall: 0.7466245937318948, test accuracy recall: 0.43068437933425796\nAccuracy f1: 0.7466245937318948, test accuracy f1: 0.43068437933425796\n=======================================================================\nEpochs: 730, loss: 0.693777924578527, test loss: 3.3338340509443376\nAccuracy acc: 0.7474781760290221, test accuracy acc: 0.43063867989056087\nAccuracy precision: 0.7474781760290221, test accuracy precision: 0.43063867989056087\nAccuracy recall: 0.7474781760290221, test accuracy recall: 0.43063867989056087\nAccuracy f1: 0.7474781760290221, test accuracy f1: 0.43063867989056087\n=======================================================================\nEpochs: 740, loss: 0.6913322399601107, test loss: 3.3555484728613685\nAccuracy acc: 0.748351259416799, test accuracy acc: 0.4305204116059379\nAccuracy precision: 0.748351259416799, test accuracy precision: 0.4305204116059379\nAccuracy recall: 0.748351259416799, test accuracy recall: 0.4305204116059379\nAccuracy f1: 0.748351259416799, test accuracy f1: 0.4305204116059379\n=======================================================================\nEpochs: 750, loss: 0.6890289339579955, test loss: 3.3780295798051534\nAccuracy acc: 0.7491600729617893, test accuracy acc: 0.4304365013315579\nAccuracy precision: 0.7491600729617893, test accuracy precision: 0.4304365013315579\nAccuracy recall: 0.7491600729617893, test accuracy recall: 0.4304365013315579\nAccuracy f1: 0.7491600729617893, test accuracy f1: 0.4304365013315579\n=======================================================================\nEpochs: 760, loss: 0.6866563352412589, test loss: 3.397423052129858\nAccuracy acc: 0.7500024365737121, test accuracy acc: 0.4303342641261498\nAccuracy precision: 0.7500024365737121, test accuracy precision: 0.4303342641261498\nAccuracy recall: 0.7500024365737121, test accuracy recall: 0.4303342641261498\nAccuracy f1: 0.7500024365737121, test accuracy f1: 0.4303342641261498\n=======================================================================\nEpochs: 770, loss: 0.6843493616384855, test loss: 3.4158452268395134\nAccuracy acc: 0.750821345675516, test accuracy acc: 0.4300218871595331\nAccuracy precision: 0.750821345675516, test accuracy precision: 0.4300218871595331\nAccuracy recall: 0.750821345675516, test accuracy recall: 0.4300218871595331\nAccuracy f1: 0.750821345675516, test accuracy f1: 0.4300218871595331\n=======================================================================\nEpochs: 780, loss: 0.682098403071412, test loss: 3.4353830979209246\nAccuracy acc: 0.751608457744453, test accuracy acc: 0.4299475832266325\nAccuracy precision: 0.751608457744453, test accuracy precision: 0.4299475832266325\nAccuracy recall: 0.751608457744453, test accuracy recall: 0.4299475832266325\nAccuracy f1: 0.751608457744453, test accuracy f1: 0.4299475832266325\n=======================================================================\nEpochs: 790, loss: 0.6799145351378374, test loss: 3.4552022856195115\nAccuracy acc: 0.7523866700539295, test accuracy acc: 0.429855404551201\nAccuracy precision: 0.7523866700539295, test accuracy precision: 0.429855404551201\nAccuracy recall: 0.7523866700539295, test accuracy recall: 0.429855404551201\nAccuracy f1: 0.7523866700539295, test accuracy f1: 0.429855404551201\n=======================================================================\nEpochs: 800, loss: 0.677687411639077, test loss: 3.473709207944358\nAccuracy acc: 0.7531843724813919, test accuracy acc: 0.42973626716604246\nAccuracy precision: 0.7531843724813919, test accuracy precision: 0.42973626716604246\nAccuracy recall: 0.7531843724813919, test accuracy recall: 0.42973626716604246\nAccuracy f1: 0.7531843724813919, test accuracy f1: 0.42973626716604246\n=======================================================================\nEpochs: 810, loss: 0.6754671318967654, test loss: 3.492617332244478\nAccuracy acc: 0.7539669755458959, test accuracy acc: 0.429571901972873\nAccuracy precision: 0.7539669755458959, test accuracy precision: 0.429571901972873\nAccuracy recall: 0.7539669755458959, test accuracy recall: 0.429571901972873\nAccuracy f1: 0.7539669755458959, test accuracy f1: 0.429571901972873\n=======================================================================\nEpochs: 820, loss: 0.6733132284238874, test loss: 3.512900902786441\nAccuracy acc: 0.7547290986505343, test accuracy acc: 0.4295637941534714\nAccuracy precision: 0.7547290986505343, test accuracy precision: 0.4295637941534714\nAccuracy recall: 0.7547290986505343, test accuracy recall: 0.4295637941534714\nAccuracy f1: 0.7547290986505343, test accuracy f1: 0.4295637941534714\n=======================================================================\nEpochs: 830, loss: 0.6712114103453385, test loss: 3.534191782006862\nAccuracy acc: 0.7554706778663803, test accuracy acc: 0.4293866576413959\nAccuracy precision: 0.7554706778663803, test accuracy precision: 0.4293866576413959\nAccuracy recall: 0.7554706778663803, test accuracy recall: 0.4293866576413959\nAccuracy f1: 0.7554706778663803, test accuracy f1: 0.4293866576413959\n=======================================================================\nEpochs: 840, loss: 0.6691541238773931, test loss: 3.5602514182486518\nAccuracy acc: 0.7562105841448546, test accuracy acc: 0.42931591854934603\nAccuracy precision: 0.7562105841448546, test accuracy precision: 0.42931591854934603\nAccuracy recall: 0.7562105841448546, test accuracy recall: 0.42931591854934603\nAccuracy f1: 0.7562105841448546, test accuracy f1: 0.42931591854934603\n=======================================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(epochs_time, epochs_loss, label=\"Loss\")\nplt.plot(epochs_time, epochs_acc_accuracy, label=\"Accuracy\")\nplt.plot(epochs_time, epochs_acc_precision, label=\"Precision\")\nplt.plot(epochs_time, epochs_acc_recall, label=\"Recall\")\nplt.plot(epochs_time, epochs_acc_f1, label=\"F1\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-27T06:22:03.723821Z","iopub.execute_input":"2023-03-27T06:22:03.724739Z","iopub.status.idle":"2023-03-27T06:22:03.973608Z","shell.execute_reply.started":"2023-03-27T06:22:03.724674Z","shell.execute_reply":"2023-03-27T06:22:03.972556Z"},"trusted":true},"execution_count":151,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"plt.plot(epochs_time, epochs_test_loss, label=\"Test loss\")\nplt.plot(epochs_time, epochs_test_acc_accuracy, label=\"Test accuracy\")\nplt.plot(epochs_time, epochs_test_acc_precision, label=\"Test precision\")\nplt.plot(epochs_time, epochs_test_acc_recall, label=\"Test recall\")\nplt.plot(epochs_time, epochs_test_acc_f1, label=\"Test f1\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {\n    \"loss\": epochs_loss,\n    \"accuracy_acc\": epochs_acc_gcn_accuracy,\n    \"accuracy_precision\":epochs_acc_gcn_precision,\n    \"accuracy_recall\": epochs_acc_gcn_recall,\n    \"accuracy_f1\": epochs_acc_gcn_f1,\n    \n    \"test_loss\": epochs_test_loss_gcn,\n    \"test_accuracy_acc\":epochs_test_acc_gcn_accuracy,\n    \"test_accuracy_precision\":epochs_test_acc_gcn_precision,\n    \"test_accuracy_recall\":epochs_test_acc_gcn_recall,\n    \"test_accuracy_f1\":epochs_test_acc_gcn_f1\n}\nga = pd.DataFrame(data)\nga.to_csv(\"gcn_hockey.csv\")\ntorch.save(model.state_dict(), 'model_gcn_hockey.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(256*256*3)","metadata":{"execution":{"iopub.status.busy":"2023-03-27T01:59:45.215690Z","iopub.execute_input":"2023-03-27T01:59:45.216058Z","iopub.status.idle":"2023-03-27T01:59:45.222638Z","shell.execute_reply.started":"2023-03-27T01:59:45.216023Z","shell.execute_reply":"2023-03-27T01:59:45.221178Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"196608\n","output_type":"stream"}]}]}